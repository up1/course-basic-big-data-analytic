{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ec25d73-b6c2-428e-b191-268dd080b2a6",
   "metadata": {},
   "source": [
    "## Workshop Spark Structured Streaming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848dbac-752c-44d9-aabf-ea67986ba790",
   "metadata": {},
   "source": [
    "## 1. Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "801c87b2-4571-449e-8cc4-52f55f634457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session, you need this to work with Spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"my streaming test app\")  \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7af6d597-3b09-4e3c-9b48-b06d982f34bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-server-01:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>my streaming test app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7cb224788b90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4289c17-2695-4e93-a1bb-49d33a49970e",
   "metadata": {},
   "source": [
    "## 2. Config path of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "014b285d-b0a5-4a57-905b-850c827d5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the path to the directory with datafiles\n",
    "PATH = \"./data/streaming/\"\n",
    "\n",
    "schema = \"timestamp int, name string, value double\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f414433c-69d1-4232-80a7-408b169feb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input part of the streaming pipeline\n",
    "# This reads all the .csv files in a given directory\n",
    "# It checks continuosly for arrival of new files\n",
    "\n",
    "input_path = PATH + \"*.csv\"\n",
    "input_stream = (spark.readStream.format(\"csv\")\n",
    "               .option(\"header\",\"true\")\n",
    "               .schema(schema)\n",
    "               .option(\"path\", input_path)\n",
    "               .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad56ea3b-ce64-4d41-971c-ca0b43115784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 16:25:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# Defines an output stream of the pipeline, this writes data to a view in memory\n",
    "# Use for testing, in a real case you would write to files and/or Kafka\n",
    "#\n",
    "# Delete the checkpoint dir if it already exists\n",
    "# ! rm -r myStreamingCheckPoint1\n",
    "\n",
    "raw_stream = (input_stream.writeStream \n",
    "             .queryName(\"data_read\")\n",
    "             .outputMode(\"append\")\n",
    "             .format(\"memory\")\n",
    "             .option(\"checkpointLocation\", \"myStreamingCheckPoint1\") \n",
    "             .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a985213-8120-4179-ad34-d55fe45285f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"data_read\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4b39466-3970-4e52-855f-d29f346aab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+\n",
      "|timestamp|name|value|\n",
      "+---------+----+-----+\n",
      "+---------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the output table\n",
    "# Run this multiple times, as you add csv files with data in the input_path directory\n",
    "\n",
    "spark.sql(\"select * from data_read\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26f393c2-68e0-45c0-a336-63c990f22611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_stream.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd26ff-cb2f-49cb-90e8-3b773ba6fda5",
   "metadata": {},
   "source": [
    "## 3. Add csv file in folder `./data/streaming/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd6dc8de-1ae1-479f-aed5-10aa755659e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+\n",
      "|timestamp|   name|value|\n",
      "+---------+-------+-----+\n",
      "|     1000| event0|  0.1|\n",
      "|     1001| event1|  0.2|\n",
      "|     1002| event2|  0.3|\n",
      "+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read again\n",
    "spark.sql(\"select * from data_read\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4302a17-d1d1-4f44-95fd-f2772a7dc8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_stream.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf936a78-e131-4fdb-8ff4-c8cf9379d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This maps the input_stream to a temporary view, so that we can work with it using SQL\n",
    "input_stream.createOrReplaceTempView(\"input_stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00354480-7dc8-4f41-a201-fe14304af555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Spark SQL to describe the aggregation and tranformation on streaming data\n",
    "df = spark.sql(\"\"\"\n",
    "select name||'_aggregated' as name_aggregated, count(*) as n_points, sum(value) sum_values \n",
    "from input_stream \n",
    "group by name\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "820399fc-1c78-481b-b4b3-7be30a445a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 16:26:03 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# Defines another output stream for the pipeline\n",
    "aggregated_stream = (df.writeStream\n",
    "                    .queryName(\"data_aggregated\")\n",
    "                    .outputMode(\"complete\")\n",
    "                    .format(\"memory\")\n",
    "                    .option(\"checkpointLocation\", \"myStreamingCheckPoint2\") \n",
    "                    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "45951ef4-0849-4fe5-aa0f-6dcb38fae665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------+\n",
      "|name_aggregated|n_points|sum_values|\n",
      "+---------------+--------+----------+\n",
      "+---------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the table with aggregated data, this is updated as new data arrives in the input pipeline\n",
    "spark.sql(\"select * from data_aggregated\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ccca9409-0332-4af2-9e6e-51eeb1d6acdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+\n",
      "|timestamp|   name|value|\n",
      "+---------+-------+-----+\n",
      "|     1000| event0|  0.1|\n",
      "|     1001| event1|  0.2|\n",
      "|     1002| event2|  0.3|\n",
      "+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from data_read\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "072f97ca-a39f-4a03-bf69-5c17458a632c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+----------+\n",
      "|   name_aggregated|n_points|sum_values|\n",
      "+------------------+--------+----------+\n",
      "| event1_aggregated|       1|       0.2|\n",
      "| event2_aggregated|       1|       0.3|\n",
      "| event0_aggregated|       1|       0.1|\n",
      "+------------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from data_aggregated\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "050605bc-19b3-4a68-a53b-ae9427d2c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 16:24:41 WARN DAGScheduler: Failed to cancel job group 37fd2b11-41a6-41c4-90b8-517925545e13. Cannot find active jobs for it.\n",
      "25/09/17 16:24:41 WARN DAGScheduler: Failed to cancel job group 37fd2b11-41a6-41c4-90b8-517925545e13. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "# stop everything\n",
    "raw_stream.stop()\n",
    "aggregated_stream.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e09109-e11c-4b0a-a409-89b8de22f2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
